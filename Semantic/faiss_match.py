import faiss
import numpy as np
import torch
import spacy
from sklearn.preprocessing import normalize
import os
import json
from transformers import AutoModel, AutoTokenizer
nlp = spacy.load("en_core_web_sm")
model_name = "bert-base-uncased"
model = AutoModel.from_pretrained(model_name)
# è®¾å®šç´¢å¼•æ–‡ä»¶è·¯å¾„
from Semantic.AdaptivePCA import tokenizer

EMBEDDINGS_DIR = "../semantic_vector/"
TEXTS_DIR = "../output/"
FAISS_INDEX_PATH = "../semantic_vector/faiss_sentence.index"
TEXT_DATA_PATH = "../output/faiss_sentences.json"



def load_all_sentence_vectors():
    """ åŠ è½½æ‰€æœ‰ `.npy` æ–‡ä»¶ä¸­çš„å¥å­å‘é‡ï¼Œå¹¶å­˜å‚¨å¯¹åº”çš„å¥å­æ–‡æœ¬ """
    all_embeddings = []
    all_sentences = []

    import re

    file_list = sorted(
        [f for f in os.listdir(EMBEDDINGS_DIR) if f.startswith("embeddings") and f.endswith(".npy")],
        key=lambda x: int(re.search(r'\d+', x).group())  # æå–æ•°å­—éƒ¨åˆ†æ’åº
    )
    for file in file_list:
        # åŠ è½½è¯­ä¹‰å‘é‡
        embeddings_path = os.path.join(EMBEDDINGS_DIR, file)
        print(embeddings_path)
        sentence_embeddings = np.load(embeddings_path)
        all_embeddings.append(sentence_embeddings)

        # è·å–å¯¹åº”çš„ JSON æ–‡ä»¶å
        idx = file.split("embeddings")[1].split(".npy")[0]  # æå–ç´¢å¼•
        sentences_path = os.path.join(TEXTS_DIR, f"processed_file_{idx}.json")
        print(sentences_path)
        if os.path.exists(sentences_path):
            with open(sentences_path, "r", encoding="utf-8") as f:
                sentences = json.load(f)
                all_sentences.extend(sentences)

    # åˆå¹¶æ‰€æœ‰ `.npy` æ–‡ä»¶çš„å‘é‡
    if all_embeddings:
        all_embeddings = np.vstack(all_embeddings)
    else:
        raise ValueError("No `.npy` embeddings found!")

    # å½’ä¸€åŒ–å‘é‡ï¼Œä½¿ FAISS ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ›´ç¨³å®š
    all_embeddings = normalize(all_embeddings, axis=1)

    return all_embeddings, all_sentences


def build_faiss_index(vectors):
    """ æ„å»ºå…¨å±€ FAISS ç´¢å¼• """
    dimension = vectors.shape[1]  # è·å–å‘é‡ç»´åº¦
    index = faiss.IndexFlatIP(dimension)  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå†…ç§¯ï¼‰
    index.add(vectors)  # æ·»åŠ æ‰€æœ‰è¯­ä¹‰å‘é‡
    return index


def save_faiss_index(index, sentence_texts):
    """ ä¿å­˜ FAISS ç´¢å¼•å’Œå¥å­æ–‡æœ¬ """
    faiss.write_index(index, FAISS_INDEX_PATH)
    with open(TEXT_DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(sentence_texts, f, ensure_ascii=False, indent=4)
    print("âœ… FAISS ç´¢å¼• & å¥å­æ–‡æœ¬å·²ä¿å­˜")


def load_faiss_index():
    """ åŠ è½½ FAISS ç´¢å¼•å’Œå¥å­æ–‡æœ¬ """
    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(TEXT_DATA_PATH):
        print("åŠ è½½å·²æœ‰çš„ FAISS ç´¢å¼•...")
        index = faiss.read_index(FAISS_INDEX_PATH)
        with open(TEXT_DATA_PATH, "r", encoding="utf-8") as f:
            sentence_texts = json.load(f)
        return index, sentence_texts
    else:
        print("æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶ï¼Œé‡æ–°æ„å»º...")
        return None, None





def query_match(question, top_n=10):
    """ ç›´æ¥ä½¿ç”¨ FAISS å…¨å±€ç´¢å¼•æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å¥å­ """

    # **2ï¸âƒ£ è®¡ç®—æŸ¥è¯¢å¥å­çš„å‘é‡**
    query_input = tokenizer([question], padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        query_embedding = model(**query_input).last_hidden_state.mean(dim=1).numpy()

    # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡ï¼Œä½¿å…¶ä¸ç´¢å¼•åŒ¹é…
    query_embedding = normalize(query_embedding, axis=1)

    # **3ï¸âƒ£ ä½¿ç”¨ FAISS è¿›è¡Œæœ€è¿‘é‚»æœç´¢**
    _, top_indices = faiss_index.search(query_embedding, top_n)

    # **4ï¸âƒ£ è·å–åŒ¹é…çš„å¥å­**
    final_top_sentences = [all_sentence_texts[i] for i in top_indices[0]]


    return final_top_sentences
if __name__=='__main__':
    faiss_index, all_sentence_texts = load_faiss_index()
    if faiss_index is None:
        # éœ€è¦é‡æ–°åŠ è½½è¯­ä¹‰å‘é‡å¹¶æ„å»ºç´¢å¼•
        all_sentence_embeddings, all_sentence_texts = load_all_sentence_vectors()
        faiss_index = build_faiss_index(all_sentence_embeddings)
        save_faiss_index(faiss_index, all_sentence_texts)
    query="latino trump 2024"
    results = query_match(query, top_n=5)
    print("\nğŸ” æœ€ç›¸ä¼¼çš„å¥å­:")
    for i, sentence in enumerate(results):
        print(f"Top {i + 1}: {sentence}")

